ğŸ§  AI Stock Multi-Agent

Multi-Agent Stock Market Analyst (portfolio-ready capstone)

This project orchestrates a team of modular AI agents to analyze a stock end-to-end â€” from data ingestion to report generation. It demonstrates modern AI orchestration, data pipelines, and LLM integration using OpenAI and local LLaMA backends.

ğŸ” Overview

The pipeline:

Ingests prices, fundamentals, and news.

Analyzes data using specialized agents (Researcher, Sentiment, Fundamental, Technical).

Synthesizes insights into a Markdown research report via the Reporter (LLM).

Optionally includes:

Prompt-v2 flow for modular LLM orchestration.

Local LLaMA backend for offline execution.

Parallel ingestion for faster data retrieval.

Visualization agent to embed charts in reports.

ğŸ“‚ Architecture
ingest/
 â”œâ”€â”€ prices.py        â†’ data/<TICKER>/prices.parquet
 â”œâ”€â”€ fundamentals.py  â†’ data/<TICKER>/fundamentals.csv
 â””â”€â”€ news.py          â†’ data/<TICKER>/news/rss_*.json

agents/
 â”œâ”€â”€ researcher.py    â†’ output/researcher/<TICKER>/
 â”œâ”€â”€ sentiment.py     â†’ output/sentiment/<TICKER>/
 â”œâ”€â”€ fundamental.py   â†’ output/fundamental/<TICKER>/
 â”œâ”€â”€ technical.py     â†’ output/technical/<TICKER>/
 â”œâ”€â”€ reporter.py      â†’ output/<TICKER>/reporter/report_*.md
 â”œâ”€â”€ visualizer.py    â†’ output/<TICKER>/charts/
 â”œâ”€â”€ prompt_manager.py
 â”œâ”€â”€ prompt_executor.py
 â””â”€â”€ orchestrator.py  â†’ Orchestrates agents and pipelines

orchestrator modes:
  - classic sequential
  - parallel ingest (`--parallel-ingest`)
  - prompt-v2 orchestrated LLM flow

âš™ï¸ Requirements

Python 3.10+ or Docker

Optional GPU (NVIDIA + CUDA)

OpenAI API key (for cloud LLMs)

Local LLaMA optional backend (--use-local-llama)

ğŸš€ Quick Start (Local)
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY=sk-...

# Verify
PYTHONPATH=src pytest -q

# Run full pipeline
PYTHONPATH=src python -m cli run AAPL 2024-06-01

ğŸ§© New CLI Features
# Help
PYTHONPATH=src python -m cli --help

# Full pipeline with OpenAI
PYTHONPATH=src python -m cli run AAPL 2024-06-01

# Use Prompt-v2 orchestration
PYTHONPATH=src python -m cli run AAPL 2024-06-01 --use-prompt-v2

# Use Local LLaMA (offline)
PYTHONPATH=src python -m cli run AAPL 2024-06-01 --use-prompt-v2 --local-llama

# Enable parallel ingestion for speed
PYTHONPATH=src python -m cli run AAPL 2024-06-01 --parallel-ingest

ğŸ§  Prompt-v2 and Local LLaMA

PromptManager defines reusable templates for agents.

PromptExecutor handles prompt execution for OpenAI or local backends.

You can run the pipeline offline with a local LLaMA stub or integration (future full model).

Example:

from agents.prompt_executor import PromptExecutor
executor = PromptExecutor(use_local_llama=True)
print(executor.execute_prompt("Summarize TSLA outlook"))

ğŸ“Š Visualization Agent

Adds plots (price trends, sentiment over time) to the final report.

PYTHONPATH=src python -m cli run AAPL 2024-06-01 --visualize


Outputs charts to:

output/<TICKER>/charts/

ğŸ§ª Testing
# Unit tests
PYTHONPATH=src pytest -q -m "not integration"

# Full test suite
PYTHONPATH=src pytest -q

ğŸ—º Roadmap (Next Steps)

Plugin API for custom agents

Real local LLaMA integration (llama-cpp / HuggingFace)

Real-time / streaming ingestion

Dashboard (Streamlit / FastAPI)

Caching + retry logic for robustness

ğŸ“œ License

MIT Â© 2025 rkendev